{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_val = 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste your implementations right here to check your result\n",
    "# (Of course you can add your classes not written here)\n",
    "def gini(sequence):\n",
    "    # 1. 先建立一個 map 記錄每個數字出現幾次\n",
    "    # 2. 用上面的資訊計算 gini\n",
    "    total = 0\n",
    "    quantity_dict = dict()\n",
    "    for item in sequence:\n",
    "        if item not in quantity_dict:\n",
    "            quantity_dict[item] = 1  # if the key not yet in the dictionary -> need to first set the key to the dict\n",
    "        else:\n",
    "            quantity_dict[item] += 1  # 對應的 key 數量加一\n",
    "        total += 1\n",
    "    \n",
    "    gini = 1\n",
    "    for key, value in quantity_dict.items():\n",
    "        gini -= ((value/total)*(value/total))\n",
    "    \n",
    "    return gini\n",
    "\n",
    "\n",
    "def entropy(sequence):\n",
    "    total = 0\n",
    "    quantity_dict = dict()\n",
    "    for item in sequence:\n",
    "        if item not in quantity_dict:\n",
    "            quantity_dict[item] = 1  # if the key not yet in the dictionary -> need to first set the key to the dict\n",
    "        else:\n",
    "            quantity_dict[item] += 1  # 對應的 key 數量加一\n",
    "        total += 1\n",
    "    \n",
    "    entropy = 0\n",
    "    for key, value in quantity_dict.items():\n",
    "        if value == 0:\n",
    "            continue  # no change to the entropy\n",
    "        elif value == total:\n",
    "            break  # entropy is 0\n",
    "        else:\n",
    "            entropy += (value/total)*np.log2(value/total)\n",
    "    \n",
    "    return -entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 21)\n",
      "(300, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1583</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>148</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>942</td>\n",
       "      <td>1651</td>\n",
       "      <td>1704</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.8</td>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>1538</td>\n",
       "      <td>2459</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>832</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.7</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>1504</td>\n",
       "      <td>1799</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.3</td>\n",
       "      <td>164</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>873</td>\n",
       "      <td>1394</td>\n",
       "      <td>1944</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>695</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.6</td>\n",
       "      <td>196</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1649</td>\n",
       "      <td>1829</td>\n",
       "      <td>2855</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0           1583     1          2.1         1  11       0          14    0.7   \n",
       "1            745     1          0.6         1   5       0          35    0.8   \n",
       "2            832     0          0.7         1   2       1          39    0.7   \n",
       "3           1175     1          1.3         0   2       0          19    0.3   \n",
       "4            695     0          0.5         0  18       1          12    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        148        7  ...        942      1651  1704    17    13          2   \n",
       "1        102        8  ...         89      1538  2459    14     1         16   \n",
       "2        103        4  ...        125      1504  1799     5     2         11   \n",
       "3        164        7  ...        873      1394  1944     9     4          9   \n",
       "4        196        2  ...       1649      1829  2855    16    13          7   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        1             0     1            1  \n",
       "1        1             1     0            0  \n",
       "2        1             0     1            0  \n",
       "3        1             1     0            0  \n",
       "4        1             1     1            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.583e+03 1.000e+00 2.100e+00 ... 1.000e+00 0.000e+00 1.000e+00]\n",
      " [7.450e+02 1.000e+00 6.000e-01 ... 1.000e+00 1.000e+00 0.000e+00]\n",
      " [8.320e+02 0.000e+00 7.000e-01 ... 1.000e+00 0.000e+00 1.000e+00]\n",
      " ...\n",
      " [1.195e+03 1.000e+00 1.100e+00 ... 1.000e+00 0.000e+00 0.000e+00]\n",
      " [6.710e+02 0.000e+00 9.000e-01 ... 1.000e+00 0.000e+00 0.000e+00]\n",
      " [1.845e+03 1.000e+00 5.000e-01 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "[1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "x_train_data = train_df.drop(columns=['price_range']).to_numpy()\n",
    "y_train_data = train_df.loc[:, 'price_range'].to_numpy()\n",
    "\n",
    "\n",
    "x_test_data = val_df.drop(columns=['price_range']).to_numpy()\n",
    "y_test_data = val_df.loc[:, 'price_range'].to_numpy()\n",
    "\n",
    "print(x_train_data)\n",
    "print(y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(x_train_data.shape[0])\n",
    "print(x_train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583.0\n",
      "745.0\n"
     ]
    }
   ],
   "source": [
    "print(x_train_data[0][0])\n",
    "print(x_train_data[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, node_id, left_node_id, right_node_id, feature_id, boundary, label):\n",
    "        self.node_id = node_id  # self node id\n",
    "        self.left_node_id = left_node_id  # left child node id\n",
    "        self.right_node_id = right_node_id  # right child node id\n",
    "        self.feature_id = feature_id  # id of the split feature\n",
    "        self.boundary = boundary  # best boundary of the split feature to get min impurity\n",
    "        self.label = label # 每個點都要記錄自己的 class label -> base on the majority on that time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None, mode='default', m=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = dict()  # map the node id to the class 'Node' information\n",
    "        self.node_number = 0\n",
    "        self.feature_number = 0\n",
    "        self.use_feature_times = dict()  # key: feature_id, times: use times\n",
    "        self.x_data = None\n",
    "        self.y_data = None\n",
    "        self.all_feature_id_list = []\n",
    "        self.mode = mode  # 'default' or 'random_forest'\n",
    "        self.m = m        # random_forest: random select 'm' features for each node\n",
    "\n",
    "    def fit(self, x_data, y_data):\n",
    "        self.feature_number = x_data.shape[1]  # x_data's column number is the feature number\n",
    "        for i in range (self.feature_number):\n",
    "            self.all_feature_id_list.append(i)\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        data_id_list = []\n",
    "        for i in range(len(x_data)):  # len(x_data) is row number\n",
    "            data_id_list.append(i)\n",
    "        \n",
    "        self.node_number += 1  # 先加完再 assign 下去\n",
    "        self.build_tree(data_id_list, depth=0, my_node_number=0)\n",
    "\n",
    "\n",
    "    def build_tree(self, data_id_list, depth, my_node_number):\n",
    "        # 1. 深度抵達了 -> do the node record and return\n",
    "        if depth == self.max_depth:\n",
    "            label_list = []\n",
    "            for id in data_id_list:\n",
    "                label_list.append(self.y_data[id])\n",
    "\n",
    "            majority_label = max(label_list, key=label_list.count)  # 拿出 label list 中出現最多次的元素\n",
    "            self.tree[my_node_number] = Node(node_id = my_node_number,\n",
    "                                             left_node_id = None,\n",
    "                                             right_node_id = None,\n",
    "                                             feature_id = None,\n",
    "                                             boundary = None,\n",
    "                                             label = majority_label)\n",
    "            return\n",
    "        \n",
    "        # # 1.5 如果 label 都一樣提早結束 -> 寫下面比寫這邊快\n",
    "        # test_pure_label_list = []\n",
    "        # for id in data_id_list:\n",
    "        #     test_pure_label_list.append(self.y_data[id])\n",
    "        # if(np.unique(test_pure_label_list).shape[0]) == 1:  # pure\n",
    "        #     majority_label = max(test_pure_label_list, key=test_pure_label_list.count)  # 拿出 label list 中出現最多次的元素\n",
    "        #     self.tree[my_node_number] = Node(node_id = my_node_number,\n",
    "        #                                      left_node_id = None,\n",
    "        #                                      right_node_id = None,\n",
    "        #                                      feature_id = None,\n",
    "        #                                      boundary = None,\n",
    "        #                                      label = majority_label)\n",
    "        #     return\n",
    "\n",
    "\n",
    "        # 2. find best feature to split the dataset\n",
    "        #    -> iterate through features\n",
    "        #    -> for each feature, find the best boundary\n",
    "        # ** for the m random feature of the random_forest **\n",
    "        feature_id_list = []\n",
    "        if self.mode == 'default':\n",
    "            feature_id_list = self.all_feature_id_list\n",
    "        elif self.mode == 'random_forest':\n",
    "            feature_id_list = np.random.choice(self.feature_number, self.m, replace=False) # range: self.feature_number, number: m, weight: default equal, replace = False(feature 不能選到兩個一樣的)\n",
    "\n",
    "        split_feature_id = 0\n",
    "        split_boundary = 0\n",
    "        if self.criterion == 'gini':\n",
    "            min_impurity = None\n",
    "            for feature_id in feature_id_list:\n",
    "                impurity, boundary = self.best_split(data_id_list, feature_id)\n",
    "                if min_impurity == None or min_impurity > impurity:\n",
    "                    min_impurity = impurity\n",
    "                    split_feature_id = feature_id\n",
    "                    split_boundary = boundary\n",
    "                    if min_impurity == 0:  # already best split -> break\n",
    "                        break\n",
    "        elif self.criterion == 'entropy':\n",
    "            max_info_gain = None\n",
    "            for feature_id in feature_id_list:\n",
    "                info_gain, boundary = self.best_split(data_id_list, feature_id)\n",
    "                if max_info_gain == None or max_info_gain < info_gain:\n",
    "                    max_info_gain = info_gain\n",
    "                    split_feature_id = feature_id\n",
    "                    split_boundary = boundary\n",
    "\n",
    "    \n",
    "        # 3. after find the best feature and its boundary to split data\n",
    "        #   (1) split the data by the (feature, boundary) as 'left_data_list' and 'right_data_list'\n",
    "        #   (2) first record the node information (feature, boundary) \n",
    "        left_data_id_list = []\n",
    "        right_data_id_list = []\n",
    "        label_list = []  # 如果這個點是 leaf node, 那要統計 majority\n",
    "        for id in data_id_list:\n",
    "            if self.x_data[id][split_feature_id] < split_boundary:  # 小於左邊\n",
    "                left_data_id_list.append(id)\n",
    "            else:                                                   # 大於等於左邊\n",
    "                right_data_id_list.append(id)\n",
    "            label_list.append(self.y_data[id])\n",
    "\n",
    "        majority_label = max(label_list, key=label_list.count)\n",
    "\n",
    "\n",
    "        # 4. 紀錄 node 資訊並遞迴下去\n",
    "        if len(left_data_id_list) == 0:  # 左邊沒有資料 -> 有切跟沒切一樣 -> 已經純了\n",
    "            self.tree[my_node_number] = Node(node_id = my_node_number,\n",
    "                                             left_node_id = None,\n",
    "                                             right_node_id = None,\n",
    "                                             feature_id = None,\n",
    "                                             boundary = None,\n",
    "                                             label = majority_label)\n",
    "\n",
    "        elif len(right_data_id_list) == 0:  # 右邊沒有資料 -> 有切跟沒切一樣 -> 已經純了\n",
    "            self.tree[my_node_number] = Node(node_id = my_node_number,\n",
    "                                             left_node_id = None,\n",
    "                                             right_node_id = None,\n",
    "                                             feature_id = None,\n",
    "                                             boundary = None,\n",
    "                                             label = majority_label)\n",
    "\n",
    "        else: # 兩邊都有資料\n",
    "            left_node_id = self.node_number\n",
    "            right_node_id = self.node_number+1\n",
    "            self.tree[my_node_number] = Node(node_id = my_node_number,\n",
    "                                                left_node_id = left_node_id,\n",
    "                                                right_node_id = right_node_id,\n",
    "                                                feature_id = split_feature_id,\n",
    "                                                boundary = split_boundary,\n",
    "                                                label = None)  # 計 majority label 沒有用\n",
    "            self.node_number += 2\n",
    "            \n",
    "            # record feature use times\n",
    "            if split_feature_id not in self.use_feature_times:\n",
    "                self.use_feature_times[split_feature_id] = 1\n",
    "            else:\n",
    "                self.use_feature_times[split_feature_id] += 1\n",
    "\n",
    "            self.build_tree(left_data_id_list, depth+1, left_node_id) # left tree\n",
    "            self.build_tree(right_data_id_list, depth+1, right_node_id)  # right tree\n",
    "\n",
    "\n",
    "    def best_split(self, data_id_list, feature_id):\n",
    "        # 根據 feature_id 在同個 feature 內找到最好的 boundary\n",
    "        # 1. take out all the data of the 'feature_id' for the boundary use\n",
    "        value_list = []\n",
    "        for data_id in data_id_list:\n",
    "            value_list.append(self.x_data[data_id][feature_id])\n",
    "        values = np.array(value_list)\n",
    "        values.sort() # from small to big\n",
    "\n",
    "\n",
    "        # 2. find the best boundary for this feature\n",
    "        if self.criterion == 'gini':\n",
    "            best_boundary = 0\n",
    "            min_impurity = None\n",
    "            for boundary in values:\n",
    "                left_label = []\n",
    "                right_label = []\n",
    "                for data_id in data_id_list:\n",
    "                    if self.x_data[data_id][feature_id] < boundary:\n",
    "                        left_label.append(self.y_data[data_id])\n",
    "                    else:\n",
    "                        right_label.append(self.y_data[data_id])\n",
    "                \n",
    "                impurity = ((len(left_label)/len(data_id_list)) * gini(left_label)) + ((len(right_label)/len(data_id_list)) * gini(right_label))\n",
    "\n",
    "                if min_impurity == None or min_impurity > impurity:\n",
    "                    min_impurity = impurity\n",
    "                    best_boundary = boundary\n",
    "                    if min_impurity == 0:  # already best boundary\n",
    "                        break\n",
    "\n",
    "            return min_impurity, best_boundary\n",
    "\n",
    "        elif self.criterion == 'entropy':\n",
    "            best_boundary = 0\n",
    "            max_info_gain = None\n",
    "            for boundary in values:\n",
    "                left_label = []\n",
    "                right_label = []\n",
    "                for data_id in data_id_list:\n",
    "                    if self.x_data[data_id][feature_id] < boundary:\n",
    "                        left_label.append(self.y_data[data_id])\n",
    "                    else:\n",
    "                        right_label.append(self.y_data[data_id])\n",
    "                \n",
    "                info_gain = entropy(left_label + right_label) - (((len(left_label)/len(data_id_list)) * entropy(left_label)) + ((len(right_label)/len(data_id_list)) * entropy(right_label)))\n",
    "\n",
    "                if max_info_gain == None or max_info_gain < info_gain:\n",
    "                    max_info_gain = info_gain\n",
    "                    best_boundary = boundary\n",
    "\n",
    "            return max_info_gain, best_boundary\n",
    "\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        predict_label = []\n",
    "        for data in x_data:\n",
    "            predict_label.append(self.get_label(data))\n",
    "        return np.array(predict_label)\n",
    "\n",
    "    def get_label(self, data):\n",
    "        # recursively call this function to get the label\n",
    "        cur_node = self.tree[0]  # start from root\n",
    "        label = None\n",
    "        while True:\n",
    "            if cur_node.left_node_id == None and cur_node.left_node_id == None: # node 是 leaf node -> 才可以判斷 class(用 majority label)\n",
    "                label = cur_node.label\n",
    "                break\n",
    "            \n",
    "            if data[cur_node.feature_id] < cur_node.boundary:  # 小於左邊\n",
    "                cur_node = self.tree[cur_node.left_node_id]\n",
    "            else:\n",
    "                cur_node = self.tree[cur_node.right_node_id]  # 大於等於右邊\n",
    "\n",
    "        return label        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tree structure\n",
    "# for key, item in clf_depth3.tree.items():\n",
    "#     print(\"node number\", key)\n",
    "#     print(\"left, right\", item.left_node_id, item.right_node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(predict_label, label):\n",
    "    acc = 0\n",
    "    for i in range(len(predict_label)):\n",
    "        if predict_label[i] == label[i]:\n",
    "            acc += 1\n",
    "    acc /= len(predict_label)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_depth3 acc:  0.92\n",
      "clf_depth10 acc:  0.93\n"
     ]
    }
   ],
   "source": [
    "clf_depth3.fit(x_train_data, y_train_data)\n",
    "print(\"clf_depth3 acc: \", get_acc(clf_depth3.predict(x_test_data), y_test_data))\n",
    "\n",
    "clf_depth10.fit(x_train_data, y_train_data)\n",
    "print(\"clf_depth10 acc: \", get_acc(clf_depth10.predict(x_test_data), y_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_gini acc:  0.92\n",
      "clf_entropy acc:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "clf_gini.fit(x_train_data, y_train_data)\n",
    "print(\"clf_gini acc: \", get_acc(clf_gini.predict(x_test_data), y_test_data))\n",
    "clf_entropy.fit(x_train_data, y_train_data)\n",
    "print(\"clf_entropy acc: \", get_acc(clf_entropy.predict(x_test_data), y_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n",
      "       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n",
      "       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n",
      "       'touch_screen', 'wifi', 'price_range'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "column_names = train_df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ram :  9\n",
      "px_height :  9\n",
      "fc :  2\n",
      "battery_power :  19\n",
      "px_width :  5\n",
      "m_dep :  2\n",
      "mobile_wt :  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_names = train_df.columns\n",
    "x = []\n",
    "h = []\n",
    "label = []\n",
    "cnt = 1\n",
    "for key, value in clf_depth10.use_feature_times.items():\n",
    "    print(column_names[key], ': ', value)\n",
    "    x.append(cnt)\n",
    "    h.append(value)\n",
    "    label.append(column_names[key])\n",
    "    cnt += 1\n",
    "\n",
    "plt.barh(x, h, tick_label=label, height=0.5)  # 改成 barh\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference\n",
    "1. video: https://www.youtube.com/watch?v=LsK-xG1cLYA&ab_channel=StatQuestwithJoshStarmer\n",
    "2. mynote: https://docs.google.com/document/d/1xV7HK57fsWBgja9RaQnMaS60vXpGfnwloXDZAknZAL0/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators, decision_tree_max_depth=1, decision_tree_criterion = \"gini\"):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.decision_tree_max_depth = decision_tree_max_depth\n",
    "        self.decision_tree_criterion = decision_tree_criterion\n",
    "        self.decision_tree_list = []\n",
    "        self.amount_of_say_list = []\n",
    "\n",
    "    def fit(self, x_data, y_data):\n",
    "        # 1. 用現在的 data 丟進去 fit 出一顆 acc != 0.5 的 decision tree(stump 應該就可以ㄌ，maxdepth = 1)\n",
    "        # 2. 把 train data 丟進去 predict\n",
    "        # 3. 根據 predict 結果(每筆資料 label 是否正確)以及\"每筆資料的 weight\" 計算出 total error\n",
    "        # 4. total error 算出 amount of say\n",
    "        # 5. amount of say 分別去更新分對分錯資料的權重並 normalize\n",
    "        # 6. 根據權重 get 出新的 dataset -> 下次選出來的 stump 才不會一樣\n",
    "        # 7. next iteration from step 1\n",
    "        data_number = x_data.shape[0]\n",
    "        new_x = x_data\n",
    "        new_y = y_data\n",
    "        x_data_weight = []\n",
    "        for i in range (data_number):  # shape[0] is how many rows\n",
    "            x_data_weight.append(1/data_number)\n",
    "\n",
    "\n",
    "        for n in range(self.n_estimators):\n",
    "            # 1. initialize the weight to be all the same\n",
    "            \n",
    "            # 2. 用現在的 data 丟進去 fit 出一顆 acc != 0.5 的 decision tree\n",
    "            #    -> *** 用 resample data fit\n",
    "            #    -> *** 用 original data predict\n",
    "            decision_tree = DecisionTree(criterion=self.decision_tree_criterion, max_depth=self.decision_tree_max_depth)\n",
    "            decision_tree.fit(new_x, new_y)\n",
    "            y_predict = decision_tree.predict(x_data) # predict on the \"original x_data\"\n",
    "            \n",
    "            # 3. calculate total error: sum of the weight of the incorrectly classified samples\n",
    "            total_error = 0 \n",
    "            for i in range(len(y_predict)):\n",
    "                if y_predict[i] != y_data[i]: # 分錯了\n",
    "                    total_error += x_data_weight[i]\n",
    "            # print(\"total error \", n, \": \", total_error)\n",
    "            \n",
    "            # 4. amount of say calculate from total error\n",
    "            # amount_of_say = (1/2) * (np.ln((1-total_error+small_val)/(total_error+small_val)))  # e 為底！！\n",
    "            amount_of_say = (1/2) * (np.log((1-total_error+small_val)/(total_error+small_val)))\n",
    "\n",
    "            # 5. calculate the new sample weight for each data\n",
    "            #    check the label\n",
    "            for i in range(len(y_predict)):\n",
    "                if y_predict[i] == y_data[i]: # 分對了\n",
    "                    x_data_weight[i] = x_data_weight[i] * np.exp(-amount_of_say)\n",
    "                elif y_predict[i] != y_data[i]: # 分錯了\n",
    "                    x_data_weight[i] = x_data_weight[i] * np.exp(amount_of_say)\n",
    "            \n",
    "            # 6. normalize the weight list to be 1 -> 有點誤差沒關係\n",
    "            weight_sum = 0\n",
    "            for weight in x_data_weight:\n",
    "                weight_sum += weight\n",
    "            for i in range(len(x_data_weight)):\n",
    "                x_data_weight[i] /= weight_sum\n",
    "            \n",
    "            # *** 7. 根據新的 weight, 從最原始的 x_data 選出新的 data_set\n",
    "            # range: 0 ~ len(data) -> get the data id\n",
    "            # 需要選出 len(x_data) 這麼多個\n",
    "            # 根據 weight 選\n",
    "            new_data_id = np.random.choice(len(x_data), len(x_data), p=x_data_weight)\n",
    "\n",
    "            # 8. 從 \"最原始的 x_data\" 拿出新的 data\n",
    "            new_x = []\n",
    "            new_y = []\n",
    "            for id in new_data_id:\n",
    "                new_x.append(x_data[id,:])\n",
    "                new_y.append(y_data[id])\n",
    "            new_x = np.stack(new_x, axis=0 ) # list of numpy array to 2D numpy array\n",
    "\n",
    "            # 9. store the decision_tree and its amount of say -> store the structure of the adaboost\n",
    "            self.decision_tree_list.append(decision_tree)\n",
    "            self.amount_of_say_list.append(amount_of_say)\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        y_predict_weight_sum = np.zeros(x_data.shape[0])  # y_predict length will be the same as x_data's number of rows\n",
    "\n",
    "        # for decision_tree in self.decision_tree_list:\n",
    "        for i in range(self.n_estimators):\n",
    "            y_predict_label = self.decision_tree_list[i].predict(x_data) # the decision tree here is already trained and we only need the predict result\n",
    "            # print(\"decision tree \", i, \"acc: \", get_acc(y_predict_label, y_test_data))\n",
    "            for j in range(len(y_predict_label)):\n",
    "                if y_predict_label[j] == 0:  # class 0\n",
    "                    y_predict_weight_sum[j] += -1 *  self.amount_of_say_list[i]\n",
    "                elif y_predict_label[j] == 1:  # class 1\n",
    "                    y_predict_weight_sum[j] += 1 *  self.amount_of_say_list[i]\n",
    "    \n",
    "        y_predict_result = []\n",
    "        for weight_sum in y_predict_weight_sum:\n",
    "            if weight_sum >= 0:  # 大等於判給 class1\n",
    "                y_predict_result.append(1)\n",
    "            else:                # 小於判給 class0\n",
    "                y_predict_result.append(0)\n",
    "        \n",
    "        return np.array(y_predict_result)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_10 = AdaBoost(n_estimators=10, decision_tree_max_depth=1) # depth == 1 error 會沒辦法往下\n",
    "adaboost_100 = AdaBoost(n_estimators=100, decision_tree_max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost_10 acc:  0.9433333333333334\n"
     ]
    }
   ],
   "source": [
    "adaboost_10.fit(x_train_data, y_train_data)\n",
    "print(\"adaboost_10 acc: \", get_acc(adaboost_10.predict(x_test_data), y_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost_100 acc:  0.97\n"
     ]
    }
   ],
   "source": [
    "adaboost_100.fit(x_train_data, y_train_data)\n",
    "y_predict_adaboost_100 = adaboost_100.predict(x_test_data)\n",
    "print(\"adaboost_100 acc: \", get_acc(adaboost_100.predict(x_test_data), y_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_gini acc:  0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "# 'random_forest version's' decision tree\n",
    "clf_gini_random = DecisionTree(criterion='gini', mode='random_forest', m=5)\n",
    "clf_gini_random.fit(x_train_data, y_train_data)\n",
    "print(\"clf_gini acc: \", get_acc(clf_gini_random.predict(x_test_data), y_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = int(np.around(max_features))  # 四捨五入轉成整數\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.select_data_num = None\n",
    "        self.decision_tree_list = []\n",
    "        \n",
    "\n",
    "    def fit(self, x_data, y_data):\n",
    "        self.select_data_num = len(x_data)  # row value\n",
    "\n",
    "        # random_select_x_data = x_data\n",
    "        # random_select_y_data = y_data\n",
    "    \n",
    "        # 1. select n estimators\n",
    "        for n in range(self.n_estimators):\n",
    "\n",
    "            # sample data number == original data size\n",
    "            if self.boostrap == True:\n",
    "                random_select_x_data_id = np.random.choice(self.select_data_num, self.select_data_num, replace=True)  # range, number to select out\n",
    "            elif self.boostrap == False:\n",
    "                random_select_x_data_id = np.random.choice(self.select_data_num, self.select_data_num, replace=False)  # can not replace the data -> select 'all' number out from the range(0, N)\n",
    "                \n",
    "            random_select_x_data = []  # reset the dataset from random select\n",
    "            random_select_y_data = []\n",
    "            for id in  random_select_x_data_id:\n",
    "                random_select_x_data.append(x_data[id])\n",
    "                random_select_y_data.append(y_data[id])\n",
    "            \n",
    "            random_select_x_data = np.stack( random_select_x_data, axis=0 )  # need to convert list of np.array to np.array\n",
    "            random_select_y_data = np.stack( random_select_y_data, axis=0 )\n",
    "            \n",
    "            decision_tree = DecisionTree(criterion='gini', mode='random_forest', m=self.max_features)\n",
    "            decision_tree.fit(random_select_x_data, random_select_y_data)\n",
    "            self.decision_tree_list.append(decision_tree)\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        y_vote = np.zeros(len(x_data))\n",
    "\n",
    "        for decision_tree in self.decision_tree_list:\n",
    "            y_pred = decision_tree.predict(x_data)\n",
    "            for i in range(len(y_pred)):\n",
    "                if y_pred[i] == 0:  # class 0 then -1\n",
    "                    y_vote[i] -= 1\n",
    "                else:               # class 1 then +1\n",
    "                    y_vote[i] += 1\n",
    "\n",
    "        y_result = []\n",
    "        for vote in y_vote:\n",
    "            if vote < 0:\n",
    "                y_result.append(0)  # 小於 0 -> class 0\n",
    "            else:\n",
    "                y_result.append(1)  # 大於等於 0 -> class 1\n",
    "\n",
    "        return np.array(y_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train_data.shape[1]))  # feature ^ 1/2\n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_10tree acc:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "clf_10tree.fit(x_train_data, y_train_data)\n",
    "print(\"clf_10tree acc: \", get_acc(clf_10tree.predict(x_test_data), y_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_100tree acc:  0.9433333333333334\n"
     ]
    }
   ],
   "source": [
    "clf_100tree.fit(x_train_data, y_train_data)\n",
    "print(\"clf_100tree acc: \", get_acc(clf_100tree.predict(x_test_data), y_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train_data.shape[1]))\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_random_features acc:  0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "clf_random_features.fit(x_train_data, y_train_data)\n",
    "print(\"clf_random_features acc: \", get_acc(clf_random_features.predict(x_test_data), y_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_all_features acc:  0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "clf_all_features.fit(x_train_data, y_train_data)\n",
    "print(\"clf_all_features acc: \", get_acc(clf_all_features.predict(x_test_data), y_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Train and tune your model on a real-world dataset\n",
    "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_your_model(data):\n",
    "#     ## Define your model and training \n",
    "#     return\n",
    "\n",
    "def train_your_model(x_train_data, y_train_data):\n",
    "    ## Define your model and training\n",
    "    # adaboost_150 = AdaBoost(n_estimators=150, decision_tree_max_depth=1)\n",
    "    # adaboost_150.fit(x_train_data, y_train_data)\n",
    "    # return adaboost_150\n",
    "    adaboost_170 = AdaBoost(n_estimators=170, decision_tree_max_depth=1)\n",
    "    adaboost_170.fit(x_train_data, y_train_data)\n",
    "    return adaboost_170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost_170 acc:  0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "my_model = train_your_model(x_train_data, y_train_data)\n",
    "my_pred = my_model.predict(x_test_data)\n",
    "print(\"adaboost_170 acc: \", get_acc(my_pred, y_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/yoona/Documents/5th_Sem/ML/CS_CS20024/HW3/HW3拷貝.ipynb Cell 55\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yoona/Documents/5th_Sem/ML/CS_CS20024/HW3/HW3%E6%8B%B7%E8%B2%9D.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39massert\u001b[39;00m y_pred\u001b[39m.\u001b[39;49mshape \u001b[39m==\u001b[39m (\u001b[39m500\u001b[39m, )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "assert y_pred.shape == (500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train 丟 dataframe 但是 predict 的時候只能丟 np.array 因為 ‘train_your_model’ return 的是 adaboost 的 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_your_model(data):\n",
    "    ## Define your model and training \n",
    "    inside_x_train_data = data.drop(columns=['price_range']).to_numpy()\n",
    "    inside_y_train_data = data.loc[:, 'price_range'].to_numpy()\n",
    "\n",
    "    adaboost_170 = AdaBoost(n_estimators=170, decision_tree_max_depth=1)\n",
    "    adaboost_170.fit(inside_x_train_data, inside_y_train_data)\n",
    "    return adaboost_170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_predict_on_val = train_your_model(train_df)  # 應該不能改 input -> valid_df 丟不進去 -> 那就 train 在 train_df 就好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost_170 acc:  0.97\n"
     ]
    }
   ],
   "source": [
    "y_pred_on_val = my_model_predict_on_val.predict(x_test_data)\n",
    "print(\"adaboost_170 acc: \", get_acc(y_pred_on_val, y_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1076</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.2</td>\n",
       "      <td>105</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>545</td>\n",
       "      <td>1300</td>\n",
       "      <td>2043</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>759</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0.3</td>\n",
       "      <td>162</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "      <td>1317</td>\n",
       "      <td>968</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1562</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>190</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>642</td>\n",
       "      <td>1533</td>\n",
       "      <td>2243</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1954</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>673</td>\n",
       "      <td>690</td>\n",
       "      <td>3438</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1989</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.8</td>\n",
       "      <td>94</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1100</td>\n",
       "      <td>1497</td>\n",
       "      <td>1665</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0           1076     0          2.5         0   3       0          14    0.2   \n",
       "1            759     0          2.5         0   3       1          39    0.3   \n",
       "2           1562     1          1.3         1   1       1           7    0.2   \n",
       "3           1954     0          0.6         1   8       0           7    0.9   \n",
       "4           1989     0          2.5         1   0       1          41    0.8   \n",
       "\n",
       "   mobile_wt  n_cores  pc  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        105        5   4        545      1300  2043     7     5         14   \n",
       "1        162        2   8        110      1317   968     6     2          2   \n",
       "2        190        5  15        642      1533  2243    12    10          6   \n",
       "3        126        3   9        673       690  3438    17    12         13   \n",
       "4         94        3  13       1100      1497  1665    17     9         12   \n",
       "\n",
       "   three_g  touch_screen  wifi  \n",
       "0        0             0     0  \n",
       "1        1             0     0  \n",
       "2        1             0     0  \n",
       "3        1             0     0  \n",
       "4        1             1     1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_df = pd.read_csv('x_test.csv')\n",
    "print(x_test_df.shape)\n",
    "\n",
    "x_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.076e+03 0.000e+00 2.500e+00 0.000e+00 3.000e+00 0.000e+00 1.400e+01\n",
      "  2.000e-01 1.050e+02 5.000e+00 4.000e+00 5.450e+02 1.300e+03 2.043e+03\n",
      "  7.000e+00 5.000e+00 1.400e+01 0.000e+00 0.000e+00 0.000e+00]\n",
      " [7.590e+02 0.000e+00 2.500e+00 0.000e+00 3.000e+00 1.000e+00 3.900e+01\n",
      "  3.000e-01 1.620e+02 2.000e+00 8.000e+00 1.100e+02 1.317e+03 9.680e+02\n",
      "  6.000e+00 2.000e+00 2.000e+00 1.000e+00 0.000e+00 0.000e+00]\n",
      " [1.562e+03 1.000e+00 1.300e+00 1.000e+00 1.000e+00 1.000e+00 7.000e+00\n",
      "  2.000e-01 1.900e+02 5.000e+00 1.500e+01 6.420e+02 1.533e+03 2.243e+03\n",
      "  1.200e+01 1.000e+01 6.000e+00 1.000e+00 0.000e+00 0.000e+00]\n",
      " [1.954e+03 0.000e+00 6.000e-01 1.000e+00 8.000e+00 0.000e+00 7.000e+00\n",
      "  9.000e-01 1.260e+02 3.000e+00 9.000e+00 6.730e+02 6.900e+02 3.438e+03\n",
      "  1.700e+01 1.200e+01 1.300e+01 1.000e+00 0.000e+00 0.000e+00]\n",
      " [1.989e+03 0.000e+00 2.500e+00 1.000e+00 0.000e+00 1.000e+00 4.100e+01\n",
      "  8.000e-01 9.400e+01 3.000e+00 1.300e+01 1.100e+03 1.497e+03 1.665e+03\n",
      "  1.700e+01 9.000e+00 1.200e+01 1.000e+00 1.000e+00 1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x_test = x_test_df.to_numpy()\n",
    "print(x_test[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 21)\n",
      "(300, 21)\n",
      "(1500, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/ppsthmjj5yj5d35sqj3jfz5m0000gn/T/ipykernel_32703/1807766568.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  trainval_df = trainval_df.append(val_df)\n"
     ]
    }
   ],
   "source": [
    "# trainval_df = pd.merge(train_df, val_df)\n",
    "# pd.concat([ironman1,ironman2])\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "\n",
    "trainval_df = train_df\n",
    "trainval_df = trainval_df.append(val_df)\n",
    "print(trainval_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainval_df.head() == train_df.head())\n",
    "print(trainval_df.tail() == val_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = train_your_model(trainval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final model pred on adaboost_170 big acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "my_pred_big = my_model.predict(x_test_data)\n",
    "print(\"final model pred on adaboost_170 big acc: \", get_acc(my_pred_big, y_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_pred.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape == (500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'y_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yoona/Documents/5th_Sem/ML/CS_CS20024/HW3/HW3拷貝.ipynb Cell 68\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yoona/Documents/5th_Sem/ML/CS_CS20024/HW3/HW3%E6%8B%B7%E8%B2%9D.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yoona/Documents/5th_Sem/ML/CS_CS20024/HW3/HW3%E6%8B%B7%E8%B2%9D.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yoona/Documents/5th_Sem/ML/CS_CS20024/HW3/HW3%E6%8B%B7%E8%B2%9D.ipynb#Y104sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39my_test.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39m\u001b[39mprice_range\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yoona/Documents/5th_Sem/ML/CS_CS20024/HW3/HW3%E6%8B%B7%E8%B2%9D.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest-set accuarcy score: \u001b[39m\u001b[39m'\u001b[39m, accuracy_score(y_test, y_pred))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1730\u001b[0m     f,\n\u001b[1;32m   1731\u001b[0m     mode,\n\u001b[1;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1738\u001b[0m )\n\u001b[1;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    856\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    858\u001b[0m             handle,\n\u001b[1;32m    859\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    860\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    861\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    862\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    863\u001b[0m         )\n\u001b[1;32m    864\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'y_test.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
    "\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** We will check your result for Question 3 manually *** (5 points)\n",
      "*** We will check your result for Question 6 manually *** (20 points)\n",
      "Approximate score range: 45.0 ~ 70.0\n",
      "*** This score is only for reference ***\n"
     ]
    }
   ],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.435  0.335  0.11   ... 0.1355 0.0775 0.0965]\n",
      " [0.625  0.495  0.155  ... 0.46   0.1945 0.34  ]\n",
      " [0.57   0.435  0.15   ... 0.3875 0.156  0.245 ]\n",
      " ...\n",
      " [0.43   0.34   0.125  ... 0.1375 0.061  0.146 ]\n",
      " [0.535  0.415  0.135  ... 0.3165 0.169  0.2365]\n",
      " [0.295  0.22   0.07   ... 0.0575 0.0295 0.035 ]]\n",
      "(332, 7)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(332,)\n",
      "<class 'numpy.ndarray'>\n",
      "['Length' 'Diameter' 'Height' 'Whole weight' 'Shucked weight'\n",
      " 'Viscera weight' 'Shell weight']\n"
     ]
    }
   ],
   "source": [
    "# def load_dataset():\n",
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "df = pd.read_csv(\n",
    "    file_url,\n",
    "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "            \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    ")\n",
    "\n",
    "df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "train_idx = range(0, len(df), 10)\n",
    "test_idx = range(1, len(df), 20)\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "\n",
    "x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "feature_names = x_train.columns.values\n",
    "x_train = x_train.values\n",
    "y_train = train_df['Target'].values\n",
    "\n",
    "x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "x_test = x_test.values\n",
    "y_test = test_df['Target'].values\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(type(x_train))\n",
    "\n",
    "print(y_train)\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "\n",
    "print(feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
